# 机器学习基石（上）

## Week 1 The Learning Problem

### 1. What is Machine Learning ？

observations --> learning --> skills

data --> ML --> skills

技巧 Skills： 表现增进 improve some performance measure

应用场景：

![1](assets/1.png)

ML使用关键条件：

![1564197326321](assets/1564197326321.png)

### 2. Applications of Machine Learning

All the regions!

方方面面都可以使用！

### 3. Components of Machine Learning

![1564199067120](assets/1564199067120.png)

### 4. Machine Learning and Other Fields

![1564199547933](assets/1564199547933.png)

![1564199644238](assets/1564199644238.png)

![1564199687467](assets/1564199687467.png)

## Week 2 Learning to Answer Yes/No

### 1. Perceptron Hypothesis Set

感知机公式：$f(x) = sign(w\cdot x + b)$

是一个线性分类器

### 2. Perceptron Learning Algorithm

$$
Loss = -\sum_{x_i} y_i(w \cdot x_i + b)\notag
$$

对w和b求偏导，利用随机梯度下降法，随机选取误分类点$(x_i,y_i)$进行更新
$$
w = w + \eta y_ix_i \\
b = b + \eta y_i \notag
$$

### 3. Guarantee of PLA

感知机只适用于线性可分的情况，只有数据线性可分时，PLA才能停止。

Novikoff定理

1）设训练集线性可分，则存在满足$||w_{opt}|| = 1$的超平面$w_{opt}\cdot x = 0$，能将训练数据完全正确分开

2）假设训练集中最大的L-2范数为$R = max||x_i||$，最小分类间隔为$\gamma > 0$，$y_i(w_{opt}\cdot x_i) \geq \gamma$，则感知机算法的误分类次数满足不等式
$$
k \leq (\frac{R}{\gamma})^2 \notag
$$

### 4. Non-Separable Data

pros: 简单实施，适用于多维

cons: 需要线性可分的先验，收敛次数不确定

Line with Noise Tolerance:
$$
\mathbf{w}_{g} \leftarrow \underset{\mathbf{w}}{\operatorname{argmin}} \sum_{n=1}^{N}\left[y_{n} \neq \operatorname{sign}\left(\mathbf{w}^{T} \mathbf{x}_{n}\right)\right] \notag
$$
这是一个NP-hard问题。

一种基于贪心的方法（PLA的一种衍生方法，pocket算法）：

![1564216297702](assets/1564216297702.png)

当数据集线性可分时，Pocket算法和PLA算法比较：

Pocket算法会慢一点，因为每一次迭代，都需要遍历$x_i$，进行计算比较；但最终结果相同。

## Week 3 Types of Learning

### 1. Learning with Different Output Space



### 2. Learning with Different Data Label

### 3. Learning with Different Protocol

### 4. Learning with Different Input Space